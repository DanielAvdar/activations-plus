"""Simple activation functions and their variants for neural networks."""

from .log_exp_softplus_variants import logish, loglog, loglogish, soft_exponential, softplus_linear_unit
from .other_variants import (
    abslu,
    celu,
    complementary_log_log,
    elu,
    erf_act,
    exp_expish,
    exp_swish,
    gelu,
    gish,
    hard_sigmoid,
    hard_swish,
    hat,
    mish,
    new_sigmoid,
    penalized_tanh,
    phish,
    prelu,
    resp,
    root2sigmoid,
    rootsig,
    selu,
    sigmoid_gumbel,
    silu,
    sin_sig,
    stanhplus,
    suish,
    swish,
    tanh_linear_unit,
    tanhsig,
)
from .polynomial_power_variants import (
    inverse_polynomial_linear_unit,
    polynomial_linear_unit,
    power_function_linear_unit,
    power_linear_unit,
)
from .relu_variants import blrelu, dual_line, lrelu, mrelu, relu, rrelu, trec
from .sigmoid_tanh_variants import hardtanh, sigmoid, softplus, softsign, sqnl, tanh, tanh_exp

__all__ = [
    # ReLU variants
    "relu",
    "lrelu",
    "blrelu",
    "rrelu",
    "trec",
    "dual_line",
    "mrelu",
    # Sigmoid/Tanh variants
    "sigmoid",
    "tanh",
    "hardtanh",
    "softsign",
    "sqnl",
    "softplus",
    "tanh_exp",
    # Polynomial/Power variants
    "polynomial_linear_unit",
    "power_function_linear_unit",
    "power_linear_unit",
    "inverse_polynomial_linear_unit",
    # Log/Exp/Softplus variants
    "loglog",
    "loglogish",
    "logish",
    "soft_exponential",
    "softplus_linear_unit",
    # Other variants
    "abslu",
    "celu",
    "complementary_log_log",
    "elu",
    "erf_act",
    "exp_expish",
    "exp_swish",
    "gelu",
    "gish",
    "hard_sigmoid",
    "hard_swish",
    "hat",
    "mish",
    "new_sigmoid",
    "penalized_tanh",
    "phish",
    "prelu",
    "resp",
    "root2sigmoid",
    "rootsig",
    "selu",
    "sigmoid_gumbel",
    "silu",
    "sin_sig",
    "stanhplus",
    "suish",
    "swish",
    "tanh_linear_unit",
    "tanhsig",
]
